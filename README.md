# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 - "Интеграция экономической системы в проект Unity и обучение ML-Agent" выполнил:
- Амбрушкевич Артем Антонович
- РИ-211002

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.

## Цель работы
Интегрировать экономическую систему в проекте Unity и обучить ML-agent.

## Задание 1
### Измените параметры файла .yaml-агента и определите какие параметры и как влияют на обучение модели.  
1. Скачал архив с проектом Unity, и добавил его к себе в Unity Hub.  
2. Активировал ранее созданное пространство в Anaconda Promt командой ``` conda activate MLAGENT ```
3. Перешел в папку с проектом командой ``` cd /d H:\ВУЗ\3 семестр\Дата сайнс в примерах и задачах\lab-5\MLA_Lab5_unity ```  
4. Командой ``` mlagents-learn Economic.yaml --run-id=Economic –-force ``` запустил обучени ML-agenta и запустил проект в Unity.  

    ![gif1](https://user-images.githubusercontent.com/97295011/204089276-5f048423-081a-41bb-9e8f-fe37de71e4ee.gif)  
5. Для того, чтобы ускорить процесс обучения - увеличил кол-во префабов 12 и снова запустил обучение ML-agenta, в результате 10000 шагов и 42.150 секунд мл-агент обучился, чего не скажешь на отметке в 5000 шагов.
    ![Снимок1](https://user-images.githubusercontent.com/97295011/204089547-fcdd48f8-0a4d-430c-985c-73b72fdb73cc.PNG)
6. Результаты обучения модели были сохранены в папку с .yaml-файлом: ```...lab-5\MLA_Lab5_unity\results\Economic```.  
7. Далее установил TensorBoard с помощью команды ``` pip install tensorflow ```.  
8. Перешел в папку с проектом командой``` cd /d H:\ВУЗ\3 семестр\Дата сайнс в примерах и задачах\lab-5\MLA_Lab5_unity ```
9. Запустил TensorBoard командой``` tensorboard --logdir=results\Economic ```, и в браузере по пути ```http://localhost:6006/``` появились следующие графики
    ![Снимок2](https://user-images.githubusercontent.com/97295011/204091381-73d443fe-0ad5-4695-8899-d814951857ff.PNG)
10. Провёл 4 обучения, в каждом меняя некоторые параметры. Далее перешел в папку с проектом командой ```cd /d H:\ВУЗ\3 семестр\Дата сайнс в примерах и задачах\lab-5\MLA_Lab5_unity``` и запустил TensorBoard командой ``` tensorboard --logdir=results\ ```, в результате в TensorBoard появились графики.
    Обозначения:  
    ![обознач](https://user-images.githubusercontent.com/97295011/204100368-bf115921-cec6-413c-a4b2-b1341ea7f859.PNG)
    * Economic-default - настройки .yaml файла по-умолчанию
    * Economic_1 - изменены следующие параметры *(по сравнению с default)*
        ``` 
        learning_rate: 2.0e-4
        gamma: 0.5
        summary_freq: 10000
        ```
        * Суммарное вознаграждение не изменилось, т.е. как было максимальным(1.0) так и осталось
        * Естевственно изменилась длина эпизода, так как поменяли параметр ``` learning_rate ```
    * Economic_2 - изменены следующие параметры *(по сравнению с default)*
        ```
        strength: 0.5
        ```  
        * я изменил коэффициент, на который умножается вознаграждение, как видно из графика ```Cumulative Reward``` , суммарное вознаграждение изменилось.  
       
            ![ec_2](https://user-images.githubusercontent.com/97295011/204100485-4223589d-8913-4242-b05c-34e9beffc49e.PNG)

    * Economic_3 - изменены следующие параметры *(по сравнению с default)*
        ```
        gamma: 0.5
        strength: 0.5
        ```
        * Посмотрим на график Policy Loss, этот график определяет величину изменения политики со временем. Политика — это элемент, определяющий действия, и в общем случае этот график должен стремиться вниз, показывая, что политика всё лучше принимает решения.
        * По сравнению с default настройками, здесь политика лучше принимает решение, а график всё время направлен вниз.
        * ![Снимок3](https://user-images.githubusercontent.com/97295011/204100175-d78145b2-54a1-4a92-bfc3-97dfee9b63ae.PNG)  
        
11. **Общие графики**  
    
    ![граф1](https://user-images.githubusercontent.com/97295011/204100354-c429b0f9-e7e7-49f6-920d-78d1419969c5.PNG)
    ![граф2](https://user-images.githubusercontent.com/97295011/204100357-25a9d22d-4142-4457-903d-329ae5d82b41.PNG)
    ![граф3](https://user-images.githubusercontent.com/97295011/204100361-3818211e-7417-47a9-9fce-1b8696b1c0a5.PNG)

## Задание 2
### Описание результатов, выведенных в TensorBoard.
* Вкладка SCALARS  
   * График ```Cumulative Reward```: это общее вознаграждение, которое максимизирует агент. Обычно нужно, чтобы оно увеличивалось, но по некоторым причинам оно может и уменьшаться. Всегда лучше максимизировать вознаграждения в интервале от 1 до -1. Если на графике вознаграждения выходят за пределы этого диапазона, то это тоже необходимо исправить.
   * График ```Episode Length```: если это значение уменьшается, то обычно это хороший знак. В конечном итоге, чем короче эпизоды, тем больше обучения. Но при необходимости длина эпизодов может увеличиваться, поэтому картина может быть и другой.
   * График ```Policy Loss```: этот график определяет величину изменения политики со временем.(объяснялось выше)
   * График ```Value Loss```: это средняя потеря функции значения. По сути она моделирует, насколько хорошо агент прогнозирует значение своего следующего состояния. Изначально это значение должно увеличиваться, а после стабилизации вознаграждения — уменьшаться
   * Графики ```Policy```: для оценки качества действий в PPO используется концепция политики, а не модели.
   * График ```Entropy```: этот график показывает величину исследования агента. Нужно, чтобы это значение уменьшалось, потому что агент узнаёт больше об окружении и ему нужно меньше исследовать.
   * График ```Learning Rate```: в данном случае это значение должно постепенно линейно уменьшаться.


## Powered by
**BigDigital Team: Denisov | Fadeev | Panov**
